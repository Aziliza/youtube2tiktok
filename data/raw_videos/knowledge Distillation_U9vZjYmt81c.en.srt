1
00:00:00,160 --> 00:00:04,080
Knowledge distillation allows large and

2
00:00:02,240 --> 00:00:05,520
complex models to transfer their learned

3
00:00:04,080 --> 00:00:07,200
knowledge to [music] smaller and more

4
00:00:05,520 --> 00:00:09,200
efficient models. In simple

5
00:00:07,200 --> 00:00:11,440
classification, we feed an image to a

6
00:00:09,200 --> 00:00:13,360
large pre-trained model such as ResNet

7
00:00:11,440 --> 00:00:14,960
or VGG. We get some probability

8
00:00:13,360 --> 00:00:16,480
distribution for the class this image

9
00:00:14,960 --> 00:00:18,160
belongs to. [music] Then we use the

10
00:00:16,480 --> 00:00:20,400
labels from our data set for guiding the

11
00:00:18,160 --> 00:00:22,400
model to predict the correct class. We

12
00:00:20,400 --> 00:00:24,240
can do it with cross entropy loss. But

13
00:00:22,400 --> 00:00:26,000
for knowledge distillation, we take a

14
00:00:24,240 --> 00:00:27,119
better and well-trained model and get

15
00:00:26,000 --> 00:00:29,279
its [music] predictions for the same

16
00:00:27,119 --> 00:00:30,800
image. These predictions are called soft

17
00:00:29,279 --> 00:00:32,640
labels because they provide more

18
00:00:30,800 --> 00:00:34,960
accurate labels for the given image

19
00:00:32,640 --> 00:00:36,800
because it has some dark knowledge. For

20
00:00:34,960 --> 00:00:38,399
this example, you can see the model is

21
00:00:36,800 --> 00:00:40,320
quite confident that this image is a

22
00:00:38,399 --> 00:00:42,480
cat. But you know a cat is more similar

23
00:00:40,320 --> 00:00:43,920
to a dog compared to a snake. So we have

24
00:00:42,480 --> 00:00:45,760
a higher probability for dog [music]

25
00:00:43,920 --> 00:00:47,760
compared to snake. This additional

26
00:00:45,760 --> 00:00:49,840
information is not present in the actual

27
00:00:47,760 --> 00:00:51,600
label which is just cat. [music] Now we

28
00:00:49,840 --> 00:00:53,600
train the smaller model to match these

29
00:00:51,600 --> 00:00:56,000
soft labels from the larger model using

30
00:00:53,600 --> 00:00:58,879
KL divergence which guides the model to

31
00:00:56,000 --> 00:00:58,879
learn better.

